# -*- coding: utf-8 -*-
"""Submission3_New.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YzN_S1JgdFt9jq7b1uJlL3j-dp0nvTlu
"""

pip install ipyplot

#mengimport library yang digunakan
import pandas as pd                                     
import numpy as np                                     
import tensorflow as tf                                 
import os                                               
import shutil
import ipyplot                                              
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm as tq
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split    
from google_drive_downloader import GoogleDriveDownloader as gdd

"""Dataset link: https://drive.google.com/file/d/1WDIfsOxZULeD8Q3vyEogmA20DYWe6sR0/view?usp=sharing"""

#mendownload dataset
gdd.download_file_from_google_drive(file_id='1WDIfsOxZULeD8Q3vyEogmA20DYWe6sR0',
                                    dest_path='content/FreshStaleImages.zip',
                                    unzip=True)

base_dir = '/content/content/FreshStaleImages'
os.listdir(base_dir)

#membuat list untuk nama file, label dan path gambar
file_name = []
label = []
file_path = []
for path, subdirs, files in os.walk(base_dir):
    for name in files:
        file_path.append(os.path.join(path, name)) 
        label.append(path.split('/')[-1])        
        file_name.append(name)

#mengubah data gambar berupa list file name, label dan path menjadi sebuah dataframe
Dataset = pd.DataFrame({"filepath":file_path,'filename':file_name,"label":label})
Dataset.groupby(['label']).size()

Dataset.head()

X= Dataset['filepath']
y= Dataset['label']

#split dataset awal menjadi data train dan test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=300)

df_train = pd.DataFrame({'filepath':X_train
              ,'label':y_train
             ,'set':'train'})

df_test = pd.DataFrame({'filepath':X_test
              ,'label':y_test
             ,'set':'test'})

print('Train size', len(df_train))
print('Test size', len(df_test))

#menampilkan jumlah gambar setiap label pada train dan test
print('===============TRAIN=============== \n')
print(df_train.groupby(['label']).size(),'\n')

print('===============TEST================ \n')
print(df_test.groupby(['label']).size(),'\n')

#menampilkan beberapa gambar di setiap label pada data train
labels = df_train['label'].values
images = df_train['filepath'].values
ipyplot.plot_class_tabs(images, labels, max_imgs_per_tab=15, img_width=150, force_b64=True)

#membuat direktori untuk train dan test
train_dir = base_dir + '/train'
test_dir = base_dir + '/test'
try:
    os.mkdir(train_dir)
    os.mkdir(test_dir)
except OSError:
    pass

#membuat direktori untuk setiap label pada direktori train dan test
label = Dataset['label'].drop_duplicates().values

for l in label:
  try:
    os.mkdir(train_dir + '/' + l)
    os.mkdir(test_dir + '/' + l)
  except OSError:
      pass

#memasukan file gambar untuk training ke setiap direktori label pada direktori train
for index, row in tq(df_train.iterrows()):
  #detect filepath
  filepath = row['filepath']
  if os.path.exists(filepath) == False:
    filepath = os.path.join(base_dir,row['label'], row['image'].split('.')[0])            
    
  #define file dest
  des_filename = filepath.split('/')[-1]
  dest_filepath = os.path.join(train_dir,row['label'],des_filename)
  
  #copy file from source to dest
  if os.path.exists(dest_filepath) == False:
    shutil.copy2(filepath,dest_filepath)

#memasukan file gambar untuk testing ke setiap direktori label pada direktori test
for index, row in tq(df_test.iterrows()):
  #detect filepath
  filepath = row['filepath']
  if os.path.exists(filepath) == False:
    filepath = os.path.join(base_dir,row['label'], row['image'].split('.')[0])            
    
  #define file dest
  des_filename = filepath.split('/')[-1]
  dest_filepath = os.path.join(test_dir,row['label'],des_filename)
  
  #copy file from source to dest
  if os.path.exists(dest_filepath) == False:
    shutil.copy2(filepath,dest_filepath)

train_datagen = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=20,
                    horizontal_flip=True,
                    shear_range = 0.2,
                    fill_mode = 'nearest')
 
test_datagen = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=20,
                    horizontal_flip=True,
                    shear_range = 0.2,
                    fill_mode = 'nearest')

train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(150, 150),
        batch_size=4,
        class_mode='categorical')
 
validation_generator = test_datagen.flow_from_directory(
        test_dir,
        target_size=(150, 150),
        batch_size=4,
        class_mode='categorical')

num_class = validation_generator.num_classes

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(num_class, activation='softmax')
])

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer=tf.optimizers.Adam(),
              metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(epoch > 50 and logs.get('accuracy') > 0.92 and logs.get('val_accuracy') > 0.92):
      print("\n Accuracy > 92% training dihentikan")
      self.model.stop_training = True

callbacks = myCallback()

history = model.fit(
    train_generator,
    steps_per_epoch=100,
    epochs=100,
    validation_data=validation_generator,
    validation_steps=5,
    verbose=2,
    callbacks=[callbacks])

#summarize history for accuracy
fig, ax = plt.subplots(figsize=(20, 10))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Training and Validation Accuracy')
plt.ylabel('value')
plt.xlabel('epoch')
plt.legend(['Training Accuracy', 'Validation Accuracy'], loc='upper left')
plt.show()

#summarize history for loss
fig, ax = plt.subplots(figsize=(20, 10))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Training and Validation Loss')
plt.ylabel('value')
plt.xlabel('epoch')
plt.legend(['Training Loss', 'Validation Los'], loc='upper left')
plt.show()

#konversi model
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

#menimpan model ke dalam format .tflite
with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model)