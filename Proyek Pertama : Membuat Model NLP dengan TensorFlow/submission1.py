# -*- coding: utf-8 -*-
"""Submission1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kQzJE9f5vPE5GKwEPlOKZ3IElqXYZ29f

#Import Library
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import f1_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from google_drive_downloader import GoogleDriveDownloader as gdd

"""#Download & Load Data
Drive link : https://drive.google.com/file/d/1Xfs7L2_49U_xuklVSugirYmSo1KCHWPe/view?usp=sharing
"""

gdd.download_file_from_google_drive(file_id='1Xfs7L2_49U_xuklVSugirYmSo1KCHWPe',
                                    dest_path='content/Question_Classification_Dataset.zip',
                                    unzip=True)

dataframe = pd.read_csv("/content/content/Question_Classification_Dataset/Question_Classification_Dataset.csv")
dataframe.head(10)

"""#Data Preprocessing"""

#drop kolom yang tidak diperlukan
dataframe = dataframe.drop(columns=['Unnamed: 0', 'Category1'])
dataframe.head()

#melakukan one-hot-encoding dan membuat dataframe baru
category = pd.get_dummies(dataframe.Category0)
dataframe_baru = pd.concat([dataframe, category], axis=1)
dataframe_baru = dataframe_baru.drop(columns='Category0')
dataframe_baru

#mengubah nilai-nilai dari dataframe ke dalam tipe data numpy array menggunakan atribut values
questions = dataframe_baru['Questions'].values
label_list = dataframe["Category0"].drop_duplicates().values.tolist()
label = dataframe_baru[labels].values
label

#membagi data train dan data test
questions_train, questions_test, label_train, label_test = train_test_split(questions, label, test_size=0.2, random_state=42)

print(len(questions_train))
print(len(questions_test))

"""#Tokenisasi"""

#tokenisasi dan mengkonversi setiap sampel menjadi sequence
tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(questions_train) 
tokenizer.fit_on_texts(questions_test)
 
sekuens_train = tokenizer.texts_to_sequences(questions_train)
sekuens_test = tokenizer.texts_to_sequences(questions_test)
 
padded_train = pad_sequences(sekuens_train) 
padded_test = pad_sequences(sekuens_test)

"""#Membangun Model"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(6, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.98 and logs.get('val_accuracy') > 0.80):
      print("\n Accuracy dan Val Accuracy sudah mencapai target training dihentikan")
      self.model.stop_training = True

callbacks = myCallback()

num_epochs = 30
history = model.fit(padded_train, label_train, epochs=num_epochs, 
                    validation_data=(padded_test, label_test), verbose=2, callbacks=[callbacks])

#Plot Loss dan Akurasi dari Trained Model
fig, ax = plt.subplots(1, 2, figsize=(10, 4))

history_df = pd.DataFrame(history.history)
history_df[['loss', 'val_loss']].plot(kind='line', ax=ax[0])
history_df[['accuracy', 'val_accuracy']].plot(kind='line', ax=ax[1]);

"""#confusion matrix"""

predictions = model.predict(padded_test)
predictions[:5]

prediction_labels = np.argmax(predictions, axis=1)
prediction_labels[:5]

labels_test = np.argmax(label_test, axis=1)
labels_test[:5]

print(classification_report(labels_test, prediction_labels, target_names=label_list))
pd.DataFrame(confusion_matrix(labels_test, prediction_labels), index=label_list, columns=label_list)